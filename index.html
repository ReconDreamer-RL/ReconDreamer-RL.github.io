<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ReconDreamer-RL</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:title" content="ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction" />
    <meta property="og:description" content="Reinforcement learning for training end-to-end autonomous driving models in closed-loop simulations is gaining growing attention. However, most simulation environments differ significantly from real-world conditions, creating a substantial simulation-to-reality (sim2real) gap. To bridge this gap, some approaches utilize scene reconstruction techniques to create photorealistic environments as a simulator. While this improves realistic sensor simulation, these methods are inherently constrained by the distribution of the training data, making it difficult to render high-quality sensor data for novel trajectories or corner case scenarios. Therefore, we propose \textit{ReconDreamer-RL}, a framework designed to integrate video diffusion priors into scene reconstruction to aid reinforcement learning, thereby enhancing end-to-end autonomous driving training. Specifically, in \textit{ReconDreamer-RL}, we introduce ReconSimulator, which combines the video diffusion prior for appearance modeling and incorporates a kinematic model for physical modeling, thereby reconstructing driving scenarios from real-world data. This narrows the sim2real gap for closed-loop evaluation and reinforcement learning. To cover more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA), which adjusts the trajectories of surrounding vehicles relative to the ego vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in). Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue of training data distribution, which is often biased toward simple straight-line movements. Experiments show that \textit{ReconDreamer-RL} improves end-to-end autonomous driving training, outperforming imitation learning methods with a 5$\times$ reduction in the Collision Ratio.
    "
    />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction" />
    <meta name="twitter:description" content="                       
Reinforcement learning for training end-to-end autonomous driving models in closed-loop simulations is gaining growing attention. However, most simulation environments differ significantly from real-world conditions, creating a substantial simulation-to-reality (sim2real) gap. To bridge this gap, some approaches utilize scene reconstruction techniques to create photorealistic environments as a simulator. While this improves realistic sensor simulation, these methods are inherently constrained by the distribution of the training data, making it difficult to render high-quality sensor data for novel trajectories or corner case scenarios. Therefore, we propose \textit{ReconDreamer-RL}, a framework designed to integrate video diffusion priors into scene reconstruction to aid reinforcement learning, thereby enhancing end-to-end autonomous driving training. Specifically, in \textit{ReconDreamer-RL}, we introduce ReconSimulator, which combines the video diffusion prior for appearance modeling and incorporates a kinematic model for physical modeling, thereby reconstructing driving scenarios from real-world data. This narrows the sim2real gap for closed-loop evaluation and reinforcement learning. To cover more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA), which adjusts the trajectories of surrounding vehicles relative to the ego vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in). Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue of training data distribution, which is often biased toward simple straight-line movements. Experiments show that \textit{ReconDreamer-RL} improves end-to-end autonomous driving training, outperforming imitation learning methods with a 5$\times$ reduction in the Collision Ratio.
   "
    />


    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>

    <style>
.carousel-control-prev,
.carousel-control-next {
    width: 50px;
    height: 50px;
    background-color: rgba(0,0,0,0.5);
    border-radius: 50%;
    top: 50%;
    transform: translateY(-50%);
    opacity: 0.7;
}

.carousel-control-prev {
    left: -60px; /* 将按钮放在图片左边外侧 */
}

.carousel-control-next {
    right: -60px; /* 将按钮放在图片右边外侧 */
}

.carousel-control-prev:hover,
.carousel-control-next:hover {
    opacity: 1;
}
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        /* 设置段落文本对齐方式 */
        p {
            text-align: justify; /* 使段落文本两端对齐 */
            text-justify: inter-word; /* 改进英文对齐，增加单词间隔 */
        }
        /* .container {
            margin-left: -100px;
        } */
        .image-row {
            display: flex;
            justify-content: center;
        }
        .figure-caption {
              margin-top: 3px; /* 顶部间距 */
              margin-bottom: -22px; /* 底部间距，你可以按需调整这个值 */
              font-size: 14px;
              text-align: center;
            }

        .image-wrapper {
            margin: 0 10px;
            text-align: center;
        }

        .caption-row {
            align-items: flex-end;
        }

        .caption-row .image-wrapper {
            margin-bottom: 20px;
        }

        .caption-row .image-wrapper p {
            margin-top: 10px;
        }
        
h3.text-center {
    font-size: 28px; /* 增大字体 */
    margin-top: 30px; /* 上方间距增加 */
    margin-bottom: 20px;
}


    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                 <b><em>ReconDreamer-RL</em>:</b> Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction
            <small>
                <!-- cvpr -->
            </small>
            </h2>

        </div>
        <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline">
                <li>Chaojun Ni<sup>*</sup><sup>1,2</sup></li>
                <li>Guosheng Zhao<sup>*</sup><sup>1,3</sup></li>
                <li>Xiaofeng Wang<sup>*</sup><sup>1</sup></li>
                <li>Zheng Zhu<sup>*</sup><sup>1</sup><span class="envelope">&#9993;</span></li>
            </ul>
            <ul class="list-inline">
                <li>Wenkang Qin<sup>1</sup></li>
                <li>Xinze Chen<sup>1</sup></li>
                <li>Guanghong Jia<sup>4</sup></li>
                <li>Guan Huang<sup>1</sup></li>
            </ul>
            <ul class="list-inline">
                <li>Wenjun Mei<sup>2</sup><span class="envelope">&#9993;</span></li>
            </ul>
            <ul class="list-inline">
                <li><sup>1</sup> GigaAI</li>
                <li><sup>2</sup> Peking University</li>
                <li><sup>3</sup> CASIA</li>
                <li><sup>4</sup> Tsinghua University</li>
            </ul>
            <!-- <ul class="list-inline">
                <li>Project Page: <a href="https://ReconDreamer-RL.github.io">https://ReconDreamer-RL.github.io</a></li>
            </ul> -->
        </div>

        </div>


<!-- href="http://arxiv.org/abs/2506.20590"  -->
        <div class="row">
            <div class="col-md-6 col-md-offset-3 text-center">
                <ul class="nav nav-pills" style="margin-top:10px; display:flex; justify-content:space-around; padding:0;">
                    <li style="flex:1;">
                        <a style="display:flex; flex-direction:column; align-items:center;">
                            <img src="img/recondreamer-rl.jpg" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li style="flex:1;">
                        <a href="https://github.com/GigaAI-research/ReconDreamer-RL" style="display:flex; flex-direction:column; align-items:center;">
                            <img src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>



        <div class="row ">
            <div class="col-md-8 col-md-offset-2 ">
                <h3 class="text-center mb-4">Abstract</h3>
                <p class="text-justify ">
                    <body>
                        <p>
                            Reinforcement learning for training end-to-end autonomous driving models in closed-loop simulations is gaining growing attention. However, most simulation environments differ significantly from real-world conditions, creating a substantial simulation-to-reality (sim2real) gap. To bridge this gap, some approaches utilize scene reconstruction techniques to create photorealistic environments as a simulator. While this improves realistic sensor simulation, these methods are inherently constrained by the distribution of the training data, making it difficult to render high-quality sensor data for novel trajectories or corner case scenarios. Therefore, we propose <em>ReconDreamer-RL</em>, a framework designed to integrate video diffusion priors into scene reconstruction to aid reinforcement learning, thereby enhancing end-to-end autonomous driving training. Specifically, in <em>ReconDreamer-RL</em>, we introduce ReconSimulator, which combines the video diffusion prior for appearance modeling and incorporates a kinematic model for physical modeling, thereby reconstructing driving scenarios from real-world data. This narrows the sim2real gap for closed-loop evaluation and reinforcement learning. To cover more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA), which adjusts the trajectories of surrounding vehicles relative to the ego vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in). Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue of training data distribution, which is often biased toward simple straight-line movements. Experiments show that <em>ReconDreamer-RL</em> improves end-to-end autonomous driving training, outperforming imitation learning methods with a 5× reduction in the Collision Ratio.

                        </body>
                        </p>
                    </body>
                </p>
            </div>
        </div>
 


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 class="text-center mb-4">Pipeline</h3>
                <p>
                    In <em>ReconDreamer-RL</em>, ReconSimulator improves appearance modeling by ReconDreamer and incorporates physical modeling to reconstruct driving scenes. In the imitation learning stage, DAA generates corner-case scenario trajectories, while CTG diversifies the ego vehicle's actions and uses ReconSimulator to render sensor data for training the policy. In the reinforcement learning stage, the policy is trained in a closed-loop environment, interacting with DAA-controlled surrounding vehicles.
                </p>
            </div>
        </div>
        <div style="text-align: center;">
            <div class="row" style="display: inline-block; text-align: left; width: auto;">
                <div class="row" style="margin-top:10px; display: inline-block; text-align: center; width: 100%;">
                    <div class="col-md-12 text-center">
                        <img src="./img/pipeline.png" class="mx-auto d-block" style="max-width: 80%; height: auto;">
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    WorldRestorer
                </h3> -->
                <h3 class="text-center mb-4">ReconSimulator</h3>
                <p>
                    The process of integrating the diffusion prior for appearance modeling. During the reconstruction of driving scenes, we first render novel trajectory view videos. These rendered videos are then processed by the DriveRestorer to enhance their visual quality, and the restored results are used to further optimize the reconstruction model. This iterative process continues until the reconstruction model converges.
                </p>
            </div>
        </div>
        <div style="text-align: center;">
            <div class="row" style="display: inline-block; text-align: left; width: auto;">
                <div class="row" style="margin-top:10px; display: inline-block; text-align: center; width: 100%;">
                    <div class="col-md-12 text-center">
                        <img src="./img/reconsimulator.png" class="mx-auto d-block" style="max-width: 45%; height: auto;">
                    </div>
                </div>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    ConsistView
                </h3> -->
                <h3 class="text-center mb-4">Dynamic Adversary Agent</h3>
                <p>
                    The pipeline of the DAA. DAA identifies the target vehicles based on their distances to the ego car from the BEV view, where the blue line represents the ego car’s trajectory and the red line represents the target vehicle. Then, DAA generates novel trajectories based on the specified interactive behavior. The generated trajectories are checked, and feasible ones are rendered using ReconSimulator.
                </p>
            </div>
        </div>
        <div style="text-align: center;">
            <div class="row" style="display: inline-block; text-align: left; width: auto;">
                <div class="row" style="margin-top:10px; display: inline-block; text-align: center; width: 100%;">
                    <div class="col-md-12 text-center">
                        <img src="./img/add10_01.png" class="mx-auto d-block" style="max-width: 35%; height: auto;">
                    </div>
                </div>
            </div>
        </div>

        <div class="col-md-8 col-md-offset-2">
                <p>
                       Examples of Dynamic Adversary Agent (DAA) controlling surrounding vehicles to simulate cut-in scenarios.
                </p>
        </div>
        <div style="text-align: center;">
            <div class="row" style="display: inline-block; text-align: left; width: auto;">
                <div class="row" style="margin-top:10px; display: inline-block; text-align: center; width: 100%;">
                    <div class="col-md-12 text-center">
                        <img src="./img/cut-in_00.png" class="mx-auto d-block" style="max-width: 60%; height: auto;">
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    ConsistView
                </h3> -->
                <h3 class="text-center mb-4">Cousin Trajectory Generator</h3>
                <p>
                    Cousin Trajectory Generator (CTG) generates cousin trajectories and performs trajectory checks to eliminate unreasonable trajectories (e.g., the pink cross marks), and finally renders the corresponding sensor data in the ReconSimulator.
                </p>
            </div>
        </div>
        <div style="text-align: center;">
            <div class="row" style="display: inline-block; text-align: left; width: auto;">
                <div class="row" style="margin-top:10px; display: inline-block; text-align: center; width: 100%;">
                    <div class="col-md-12 text-center">
                        <img src="./img/ego-change7_00.png" class="mx-auto d-block" style="max-width: 60%; height: auto;">
                    </div>
                </div>
            </div>
        </div>




        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    ConsistView
                </h3> -->
                <h3 class="text-center mb-4">Qualitative Examples</h3>
                <p>
                    Comparison of different methods in challenging corner cases, with collisions highlighted by orange boxes.
                </p>
            </div>
        </div>
        <div style="text-align: center;">
            <div class="row" style="display: inline-block; text-align: left; width: auto;">
                <div class="row" style="margin-top:10px; display: inline-block; text-align: center; width: 100%;">
                    <div class="col-md-12 text-center">
                        <img src="img\visual6_00.png" class="mx-auto d-block" style="max-width: 70%; height: auto;">
                    </div>
                </div>
            </div>
        </div>

        
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 class="text-center mb-4">Coarse 3D World VS <em>WonderFree</em></h3>
                <p>
                     The video below shows a comparison between <em>WonderFree</em>'s refined 3D world and the coarse 3D world.
                </p>
            </div> -->

        <!-- <div class="container" style="margin-top:10px;">
            <div class="row justify-content-center">
                <div class="col-md-8">
                    <video width="100%" height="auto" controls autoplay muted>
                        <source src="./img1/2.mp4" type="video/mp4">
                        您的浏览器不支持HTML5视频播放。
                    </video>
                </div>
            </div>
        </div>
        </div> -->

    </div>
    
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.2/dist/js/bootstrap.bundle.min.js"></script>
            

</body>

</html>
